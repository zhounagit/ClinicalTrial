import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.mllib.fpm.AssociationRules;
import org.apache.spark.mllib.fpm.FPGrowth;
import org.apache.spark.mllib.fpm.FPGrowthModel;

import java.io.FileOutputStream;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.*;


public class MiningFrequencyAndAssociation {
    public static void main(String[] args) {
        String inputFile = "/Users/nazhou/Downloads/testdata.txt";
        SparkConf conf = new SparkConf ().setAppName ("FP-Growth Example").setMaster ("local");
        JavaSparkContext sc = new JavaSparkContext (conf);
        JavaRDD<String> data = sc.textFile (inputFile).cache ();
        JavaRDD<List<String>> transactions = data.map (
                (Function<String, List<String>>) line -> {
                    String[] parts = line.split (" ");
                    return Arrays.asList (parts);
                }
        );

        FPGrowth fpg = new FPGrowth ()
                .setMinSupport (0.3)
                .setNumPartitions (3);

        FPGrowthModel<String> model = fpg.run (transactions);
        Map<String, Integer> freqMap = new HashMap<> ();
        Map<String, Double> confMap = new HashMap<> ();

        try {
            PrintWriter completeConfidence = new PrintWriter (new FileOutputStream (("/Users/nazhou/Downloads/output/Confidence.csv"), false));
            PrintWriter completeFrequency = new PrintWriter (new FileOutputStream (("/Users/nazhou/Downloads/output/FrequencyPatterns.csv"), false));
            PrintWriter gephiConfidence = new PrintWriter (new FileOutputStream (("/Users/nazhou/Downloads/output/gephiConfidence.csv"), false));
            CreateColumnsHeader(completeFrequency, completeConfidence,gephiConfidence);

            //Mining frequent patterns
            for (FPGrowth.FreqItemset<String> itemset : model.freqItemsets ().toJavaRDD ().collect ()) {

                freqMap.put (itemset.javaItems ().toString (), (int) itemset.freq ());
            }
            Map sortedFreq = SortByValues(freqMap);
            CollectValues ((HashMap<String, Integer>)sortedFreq, completeFrequency);

            //Discover association rules on items > Min Support
            double minConfidence = 0.5;
            for (AssociationRules.Rule<String> rule : model.generateAssociationRules (minConfidence).toJavaRDD ().collect ()) {
                confMap.put (rule.javaAntecedent () + "->" + rule.javaConsequent (), rule.confidence ());

                String antecedent = CleanString (rule.javaAntecedent ().toString ());
                String consequent = CleanString (rule.javaConsequent ().toString ());
                gephiConfidence.append (antecedent);
                gephiConfidence.append(",");
                gephiConfidence.append (consequent);
                gephiConfidence.append(",");
                gephiConfidence.append (Double.toString (rule.confidence ()));
                gephiConfidence.append(",");
                gephiConfidence.append("Directed");
                gephiConfidence.append("\n");
            }
            Map sortedConf = SortByValues (confMap);
            CollectValues ((HashMap<String, Integer>)sortedConf, completeConfidence);

            gephiConfidence.close ();
            completeFrequency.close ();
            completeConfidence.close ();

        } catch (IOException e) {
            e.printStackTrace ();
        }
    }

    private static void CreateColumnsHeader(PrintWriter completeFrequency, PrintWriter completeConfidence, PrintWriter gephiConfidence) {
        completeFrequency.append("ItemSet");
        completeFrequency.append(",");
        completeFrequency.append("Frequency");
        completeFrequency.append(",");
        completeFrequency.append("\n");

        completeConfidence.append("Source -> Target");
        completeConfidence.append(",");
        completeConfidence.append("Confidence");
        completeConfidence.append(",");
        completeConfidence.append("\n");

        gephiConfidence.append ("Source");
        gephiConfidence.append(",");
        gephiConfidence.append ("Target");
        gephiConfidence.append(",");
        gephiConfidence.append ("Confidence");
        gephiConfidence.append(",");
        gephiConfidence.append ("Type");
        gephiConfidence.append("\n");
    }

    private static void CollectValues(HashMap sortedFreqMap, PrintWriter writer) {
        Set s = sortedFreqMap.entrySet ();
        Iterator iterator = s.iterator ();
        while (iterator.hasNext ()) {
            Map.Entry entry = (Map.Entry) iterator.next ();
            String key = CleanString(entry.getKey ().toString ());
            writer.append (key);
            writer.append (",");
            String value = CleanString(entry.getValue ().toString ());
            writer.append (value);
            writer.append (",");
            writer.append ("\n");
        }
    }


    public static String CleanString(String s){
        return s.replace (",", "").replace ("[","").replace ("]","");
    }

    public static <K extends Comparable,V extends Comparable> Map<K,V> SortByValues(Map<K,V> map){
        List<Map.Entry<K,V>> entries = new LinkedList<Map.Entry<K,V>>(map.entrySet());

        Collections.sort(entries, (o1, o2) -> o2.getValue().compareTo(o1.getValue()));

        Map<K,V> sortedMap = new LinkedHashMap<K,V>();
        for(Map.Entry<K,V> entry: entries){
            sortedMap.put(entry.getKey(), entry.getValue());
        }

        return sortedMap;
    }

    //Sorting map by key is easier than by values because keys are unique, values can be duplicated or null
    public static <K extends Comparable,V extends Comparable> Map<K,V> SortByKeys(Map<K,V> map){
        List<K> keys = new LinkedList<K>(map.keySet());
        Collections.sort(keys);
        Map<K,V> sortedMap = new LinkedHashMap<K,V>();
        for(K key: keys){
            sortedMap.put(key, map.get(key));
        }

        return sortedMap;
    }

}


